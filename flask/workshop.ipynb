{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pprint import pprint\n",
    "\n",
    "#https://mm-gpt-chat-prod.azurewebsites.net\n",
    "resp = requests.post(\"http://127.0.0.1:5000/chat\", json={\"matter_id\": \"12766106\", \"question\": \"Who is the plaintiff in this case?\", \"chat_history\": \"\"}).json()\n",
    "pprint(resp)\n",
    "\n",
    "resp = requests.post(\"http://127.0.0.1:5000/chat\", json={\"matter_id\": \"12766106\", \"question\": \"What happened to him?\", \"chat_history\": resp['updated_chat_history']}).json()\n",
    "pprint(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Gathering\n",
    "The first part of the vectorstore QA agent is creating the vector store. To do this, we scrape the documentation we want the agent to have access to and put it in a vector store. In this example, we used chroma, which allows you to create local vector stores, but for Morgan and Morgan we used a hosted version called Pinecone capable of hosting enormous amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup\n",
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "import pickle\n",
    "\n",
    "urls = [\"https://docs.manim.community/en/stable/\"]\n",
    "\n",
    "# loader = RecursiveUrlLoader(url=\"https://docs.manim.community/en/stable/\", max_depth=3, extractor=lambda x: Soup(x, \"html.parser\").text)\n",
    "# docs = loader.load()\n",
    "\n",
    "with open(\"manim_docs.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "print(f\"{len(docs)} documents loaded\")\n",
    "pprint(docs[8].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "split_docs = text_splitter.split_documents(docs)\n",
    "\n",
    "db = Chroma.from_documents(split_docs, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: QA Agent\n",
    "Now that we have our documentation in a vector store, we can use LangChain to create an agent capable of answering questions about these documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(model=\"gpt-4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.pydantic_v1 import BaseModel, Field, validator\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\n",
    "\n",
    "class DocsResponse(BaseModel):\n",
    "    explanation: str = Field(description=\"The explanation answering the user's question\")\n",
    "    example: str = Field(description=\"A code example to support the explanation\")\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=DocsResponse)\n",
    "fixing_parser = OutputFixingParser.from_llm(parser=parser, llm=chat)\n",
    "\n",
    "prompt_template = \"\"\"You are an AI assistant helping a user with a question about writing code with Manim. Answer the user's question using the documentation below and provide a code example to support your explanation.\n",
    "\n",
    "FORMAT: {format_instructions}\n",
    "\n",
    "DOCUMENTATION:\n",
    "{context}\n",
    "\n",
    "REQUEST: \n",
    "{question}\n",
    "\n",
    "Answer the user's question according to the FORMAT above:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"], partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain_type_kwargs = {\"prompt\": PROMPT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-4\"), chain_type=\"stuff\", retriever=db.as_retriever(), chain_type_kwargs=chain_type_kwargs, return_source_documents=True)\n",
    "\n",
    "answer = qa_chain({\"query\": \"How can I animate a circle in Manim?\"})\n",
    "\n",
    "parsed_answer = fixing_parser.parse(answer['result'])\n",
    "\n",
    "print(parsed_answer.explanation)\n",
    "pprint(parsed_answer.example)\n",
    "print(answer['source_documents'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Morgan_and_Morgan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
